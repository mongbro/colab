{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09. news",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KadzPLumXjCarXJpJHeEC51E8MyDCEY-",
      "authorship_tag": "ABX9TyNqNTrPIgSBPrjs+UW5+kgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mongbro/colab/blob/main/09_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppGL1OawacB"
      },
      "source": [
        "### keras RNN으로 BBC 기사 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaoW5i0whKR"
      },
      "source": [
        "1. 패키지 수입 및 파라미터 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM3blB2GcaBL"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import numpy as np\r\n",
        "import csv\r\n",
        "import nltk # natural language tool kit\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM, Dropout, Embedding\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from time import time\r\n",
        "from sklearn.metrics import confusion_matrix, f1_score\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZGC6qZ_yaUr"
      },
      "source": [
        "# 파라미터 지정\r\n",
        "MY_VOCAB = 5000   # 내가 사용할 단어의 수, 제일 많이 사용된 단어\r\n",
        "MY_EMBED = 64     # 임베딩 차원\r\n",
        "MY_HIDDEN = 100   # LSTM 셀의 규모\r\n",
        "MY_LEN = 200      # 기사의 길이\r\n",
        "# 원본 => 5000, 64, 100, 200\r\n",
        "\r\n",
        "MY_SPLIT = 0.8    # 학습용 데이터의 비율\r\n",
        "MY_SAMPLE = 123   # 샘플 기사\r\n",
        "MY_EPOCH = 100    # 반복 학습 수\r\n",
        "TRAIN_MODE = 1    # 학습 모드와 평가 모드 선택\r\n",
        "# 원본 => 0.8, 123, 100, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFTyPS7C0d2Y"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrpUXHot0gD3",
        "outputId": "e5816282-8b40-48e2-fd32-91d1f4b4f2c7"
      },
      "source": [
        "# 제외어 (stopword) 설정\r\n",
        "nltk.download('stopwords')\r\n",
        "MY_STOP = set(nltk.corpus.stopwords.words('english'))\r\n",
        "\r\n",
        "print('영어 단어 제외')\r\n",
        "print(MY_STOP)\r\n",
        "print('제외어 개수 :', len(MY_STOP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "영어 단어 제외\n",
            "{'won', 'has', 'was', 'will', 'to', 'weren', 'in', 'all', \"you'll\", 'our', \"couldn't\", 'you', 'before', 'having', 'being', 'about', 'ma', 'myself', 'some', \"should've\", 'its', 'were', 'been', 'over', 'didn', 'under', 'shouldn', 'against', \"you'd\", 'am', 'what', 'hadn', \"you've\", 'does', 'no', 'too', 'now', 'same', 'from', 'while', 'again', 'during', 'both', \"weren't\", 'other', 'he', 'the', \"didn't\", 'most', 'those', 'who', 'wasn', 'why', 'if', 'ourselves', 'll', 'because', \"aren't\", 'his', 'couldn', 'it', 'hers', 'which', 'but', 'here', 'y', \"that'll\", \"you're\", 'for', 'any', 'than', \"isn't\", 'above', 'her', 'more', 'shan', \"won't\", 'into', \"haven't\", 'down', 'him', 've', \"doesn't\", 't', 'just', 'yours', 'through', 'an', \"hasn't\", 're', 'how', 'haven', 'me', 'm', 'doing', \"hadn't\", \"mightn't\", 'wouldn', 'i', 'of', 'my', 'a', 'at', 'out', 'after', 'that', 'with', 's', 'can', 'aren', 'should', 'their', 'your', 'don', 'mightn', 'be', 'and', 'few', 'as', 'ain', 'when', 'ours', \"wouldn't\", 'yourselves', 'not', 'themselves', 'only', 'theirs', 'where', 'below', 'is', 'then', 'whom', 'o', 'did', 'mustn', 'further', 'doesn', 'each', 'such', 'once', 'herself', 'are', 'very', \"needn't\", 'yourself', 'between', 'had', 'nor', \"shan't\", 'so', 'on', 'by', 'up', 'until', 'have', 'this', 'himself', \"wasn't\", 'needn', 'we', \"don't\", \"shouldn't\", 'they', 'off', 'these', 'there', \"it's\", 'them', 'or', 'own', 'isn', 'she', 'd', \"she's\", 'do', \"mustn't\", 'itself', 'hasn'}\n",
            "제외어 개수 : 179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAWYofJ12gxH"
      },
      "source": [
        "# 데이터 보관 창고\r\n",
        "original = []\r\n",
        "articles = []\r\n",
        "labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLHVz1oR262X",
        "outputId": "5be737c1-bce5-4221-a4d8-54e8f7005471"
      },
      "source": [
        "# BBC 파일 읽고 처리\r\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/bbc-text.csv', 'r') as file:\r\n",
        "    # 칼럼 이름 읽기\r\n",
        "    reader = csv.reader(file)\r\n",
        "    next(reader)\r\n",
        "\r\n",
        "    # 기사 하나씩 처리\r\n",
        "    for row in reader:\r\n",
        "        # 카테고리 저장\r\n",
        "        labels.append(row[0])\r\n",
        "\r\n",
        "        # 원본 기사 저장\r\n",
        "        original.append(row[1])\r\n",
        "\r\n",
        "        # 제외어 삭제 하기\r\n",
        "        news = row[1]\r\n",
        "        for word in MY_STOP:\r\n",
        "            mask = ' ' + word + ' '\r\n",
        "            news = news.replace(mask, ' ')\r\n",
        "        # 제외어를 뺀 기사 저장\r\n",
        "        articles.append(news)\r\n",
        "        \r\n",
        "print('처리한 기사 수 :', len(articles))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리한 기사 수 : 2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8uuXJCABh6K",
        "outputId": "61721421-26c7-4eee-a0bf-e6a681419436"
      },
      "source": [
        "# 샘플 기사 출력\r\n",
        "print('샘플 기사 원본 >> ')\r\n",
        "print(original[MY_SAMPLE])\r\n",
        "print(labels[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(original[MY_SAMPLE].split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 원본 >> \n",
            "screensaver tackles spam websites net users are getting the chance to fight back against spam websites  internet portal lycos has made a screensaver that endlessly requests data from sites that sell the goods and services mentioned in spam e-mail. lycos hopes it will make the monthly bandwidth bills of spammers soar by keeping their servers running flat out. the net firm estimates that if enough people sign up and download the tool  spammers could end up paying to send out terabytes of data.   we ve never really solved the big problem of spam which is that its so damn cheap and easy to do   said malte pollmann  spokesman for lycos europe.  in the past we have built up the spam filtering systems for our users   he said   but now we are going to go one step further.    we ve found a way to make it much higher cost for spammers by putting a load on their servers.  by getting thousands of people to download and use the screensaver  lycos hopes to get spamming websites constantly running at almost full capacity. mr pollmann said there was no intention to stop the spam websites working by subjecting them with too much data to cope with. he said the screensaver had been carefully written to ensure that the amount of traffic it generated from each user did not overload the web.  every single user will contribute three to four megabytes per day   he said   about one mp3 file.  but  he said  if enough people sign up spamming websites could be force to pay for gigabytes of traffic every single day. lycos did not want to use e-mail to fight back  said mr pollmann.  that would be fighting one bad thing with another bad thing   he said.  the sites being targeted are those mentioned in spam e-mail messages and which sell the goods and services on offer.  typically these sites are different to those that used to send out spam e-mail and they typically only get a few thousand visitors per day. the list of sites that the screensaver will target is taken from real-time blacklists generated by organisations such as spamcop. to limit the chance of mistakes being made  lycos is using people to ensure that the sites are selling spam goods. as these sites rarely use advertising to offset hosting costs  the burden of high-bandwidth bills could make spam too expensive  said mr pollmann. sites will also slow down under the weight of data requests. early results show that response times of some sites have deteriorated by up to 85%. users do not have to be registered users of lycos to download and use the screensaver. while working  the screensaver shows the websites that are being bothered with requests for data. the screensaver is due to be launched across europe on 1 december and before now has only been trialled in sweden. despite the soft launch  mr pollmann said that the screensaver had been downloaded more than 20 000 times in the last four days.  there s a huge user demand to not only filter spam day-by-day but to do something more   he said  before now users have never had the chance to be a bit more offensive.\n",
            "tech\n",
            "총 단어 수 : 536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd0JmTafEVrq",
        "outputId": "ac9c21c0-8c3c-46ee-c315-d4659163a911"
      },
      "source": [
        "# 제외어 처리 결과\r\n",
        "print('샘플 기사 제외어 삭제본 >> ')\r\n",
        "print(articles[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(articles[MY_SAMPLE].split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 제외어 삭제본 >> \n",
            "screensaver tackles spam websites net users getting chance fight back spam websites  internet portal lycos made screensaver endlessly requests data sites sell goods services mentioned spam e-mail. lycos hopes make monthly bandwidth bills spammers soar keeping servers running flat out. net firm estimates enough people sign download tool  spammers could end paying send terabytes data.   never really solved big problem spam damn cheap easy   said malte pollmann  spokesman lycos europe.  past built spam filtering systems users   said   going go one step further.    found way make much higher cost spammers putting load servers.  getting thousands people download use screensaver  lycos hopes get spamming websites constantly running almost full capacity. mr pollmann said intention stop spam websites working subjecting much data cope with. said screensaver carefully written ensure amount traffic generated user overload web.  every single user contribute three four megabytes per day   said   one mp3 file.   said  enough people sign spamming websites could force pay gigabytes traffic every single day. lycos want use e-mail fight back  said mr pollmann.  would fighting one bad thing another bad thing   said.  sites targeted mentioned spam e-mail messages sell goods services offer.  typically sites different used send spam e-mail typically get thousand visitors per day. list sites screensaver target taken real-time blacklists generated organisations spamcop. limit chance mistakes made  lycos using people ensure sites selling spam goods. sites rarely use advertising offset hosting costs  burden high-bandwidth bills could make spam expensive  said mr pollmann. sites also slow weight data requests. early results show response times sites deteriorated 85%. users registered users lycos download use screensaver. working  screensaver shows websites bothered requests data. screensaver due launched across europe 1 december trialled sweden. despite soft launch  mr pollmann said screensaver downloaded 20 000 times last four days.  huge user demand filter spam day-by-day something   said  users never chance bit offensive.\n",
            "총 단어 수 : 303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyXn1KQnGEaR",
        "outputId": "4420dd2a-1de8-4704-9a9d-b1ade8ae8683"
      },
      "source": [
        "# Tokenizer 처리\r\n",
        "A_token = Tokenizer(num_words = MY_VOCAB,\r\n",
        "                    oov_token = 'oov')\r\n",
        "# oov란? 제외되지 않은 단어 중에서 사용 빈도가 적어서 5000개 단어에 포함하지 않는 단어들\r\n",
        "#                        MY_VOCAB가 적어질수록 oov가 늘어난다\r\n",
        "\r\n",
        "A_token.fit_on_texts(articles)\r\n",
        "A_tokenized = A_token.texts_to_sequences(articles)  # => 텍스트를 숫자로 변환(hash function)\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(A_token.sequences_to_texts([[1]]))      # 1은 어떤 단어인가? => 'oov'(생략된 단어)\r\n",
        "                                              # MY_VOCAB가 적어질수록 1이 늘어난다\r\n",
        "print(A_token.sequences_to_texts([[1259]]))   # 1140은 어떤 단어인가? => 'the'\r\n",
        "print(A_token.texts_to_sequences(['the']))    # 'the'는 어떤 숫자인가? => 1173\r\n",
        "print(A_token.texts_to_sequences(['oov']))    # 'oov'는 어떤 숫자인가? => 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['oov']\n",
            "['welsh']\n",
            "[[1219]]\n",
            "[[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPJApClH6Q4",
        "outputId": "a8a78c8f-a04d-458c-edeb-ccd16d8010fd"
      },
      "source": [
        "# Token  처리 결과 출력\r\n",
        "sample = A_tokenized[MY_SAMPLE]\r\n",
        "print(sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3171, 1, 816, 878, 115, 136, 382, 347, 716, 28, 816, 878, 228, 1, 3172, 27, 3171, 1, 4868, 203, 568, 733, 1771, 126, 4025, 816, 260, 395, 3172, 700, 21, 1647, 3629, 2849, 2607, 1, 2326, 2551, 453, 2919, 569, 115, 63, 2291, 381, 7, 1160, 780, 1860, 2607, 11, 92, 1571, 1051, 1, 203, 281, 154, 1, 138, 364, 816, 1, 2225, 847, 2, 1, 1, 178, 3172, 139, 255, 1109, 816, 1, 726, 136, 2, 52, 60, 10, 818, 3792, 195, 41, 21, 56, 494, 245, 2607, 1363, 1, 2551, 382, 1021, 7, 780, 70, 3171, 3172, 700, 23, 1, 878, 3993, 453, 343, 322, 1394, 3, 1, 2, 3428, 582, 816, 878, 297, 1, 56, 203, 2297, 2404, 2, 3171, 2709, 1069, 660, 812, 1287, 3885, 1539, 1, 466, 224, 503, 1539, 1, 31, 96, 1, 681, 111, 2, 10, 1899, 912, 2, 381, 7, 1160, 1, 878, 11, 722, 256, 1, 1287, 224, 503, 111, 3172, 79, 70, 260, 395, 716, 28, 2, 3, 1, 4, 1604, 10, 823, 455, 158, 823, 455, 2, 568, 2179, 4025, 816, 260, 395, 891, 733, 1771, 126, 220, 3677, 568, 316, 86, 1051, 816, 260, 395, 3677, 23, 1, 1453, 681, 111, 415, 568, 3171, 760, 367, 189, 14, 1, 3885, 1595, 1, 1376, 347, 3755, 27, 3172, 200, 7, 660, 568, 848, 816, 1771, 568, 3258, 70, 2068, 4063, 4054, 416, 3793, 77, 3629, 2849, 11, 21, 816, 1730, 2, 3, 1, 568, 6, 1430, 4185, 203, 4868, 251, 664, 65, 910, 231, 568, 1, 3261, 136, 2820, 136, 3172, 780, 70, 3171, 297, 3171, 570, 878, 1, 4868, 203, 3171, 269, 633, 383, 139, 35, 233, 1, 2644, 193, 4458, 610, 3, 1, 2, 3171, 1966, 264, 32, 231, 12, 96, 276, 430, 1539, 379, 1, 816, 111, 2985, 111, 323, 2, 136, 281, 347, 651, 4064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-nD_bkAPlaz",
        "outputId": "5e293288-9af5-4511-8d62-18a64843950e"
      },
      "source": [
        "# 기사 통계 내기\r\n",
        "# 제외어 빼고 제일 긴, 짧은 기사 구하기\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "print('제일 짧은 기사 :', shortest)\r\n",
        "\r\n",
        "# 모든 기사에서 제외어를 빼고 사용된 모든 단어 수\r\n",
        "print('총 단어 수 :', len(A_token.word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 2279\n",
            "제일 짧은 기사 : 50\n",
            "총 단어 수 : 29698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZENVw3UR4Hb",
        "outputId": "6089cebf-485b-44cb-8038-ab18b4cfdbb8"
      },
      "source": [
        "# 기사 길이 맞추기\r\n",
        "# MY_LEN보다 긴건 자르고 짧은건 무언가(0)를 더해준다\r\n",
        "A_tokenized = pad_sequences(A_tokenized,\r\n",
        "                            maxlen = MY_LEN,\r\n",
        "                            padding = 'post',     # 200단어보다 짧은 기사는 뒷부분을 0으로 패딩처리\r\n",
        "                            truncating = 'post')  # 200단어보다 긴 기사는 뒷부분 삭제\r\n",
        "\r\n",
        "# 기사 길이 확인\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "print('제일 짧은 기사 :', shortest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 200\n",
            "제일 짧은 기사 : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0xjopnMUJ-E",
        "outputId": "65d07212-975d-45ac-d189-14cbadad670b"
      },
      "source": [
        "# 라벨 tokenization\r\n",
        "C_token = Tokenizer()\r\n",
        "C_token.fit_on_texts(labels)      # hash function\r\n",
        "C_tokenized = C_token.texts_to_sequences(labels)\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(C_token.word_index)\r\n",
        "print(C_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgCKe-a8WePz",
        "outputId": "447f8efe-5627-4497-a688-1875785de49d"
      },
      "source": [
        "# 데이터 4분할\r\n",
        "C_tokenized = np.array(C_tokenized)   # 기존의 C_tokenized는 list형식이다\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized,\r\n",
        "                                                    C_tokenized,\r\n",
        "                                                    train_size = MY_SPLIT,\r\n",
        "                                                    shuffle = False)\r\n",
        "\r\n",
        "# 데이터 모양 확인\r\n",
        "print('학습용 입력 데이터 모양 :', X_train.shape)\r\n",
        "print('학습용 출력 데이터 모양 :', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터 모양 :', X_test.shape)\r\n",
        "print('평가용 출력 데이터 모양 :', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 모양 : (1780, 200)\n",
            "학습용 출력 데이터 모양 : (1780, 1)\n",
            "평가용 입력 데이터 모양 : (445, 200)\n",
            "평가용 출력 데이터 모양 : (445, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EarbCbaTejNU"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEkrvtRXehTl",
        "outputId": "94eaa3cb-db0e-4e0d-bd60-f5a31b09aef9"
      },
      "source": [
        "# RNN 구현\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(input_dim = MY_VOCAB,       # 1 * 5000 행렬에 5000 * 64행렬을 곱해서\r\n",
        "                    output_dim = MY_EMBED))     # 1 * 64 행렬로 만든다.\r\n",
        "\r\n",
        "model.add(Dropout(rate = 0.5))  # 임의의 뉴런의 출력을 일부러 0으로 만드는 작업\r\n",
        "                                # 왜? => 과적합을 방지하기 위해서\r\n",
        "\r\n",
        "model.add(Bidirectional(LSTM(units = MY_HIDDEN)))\r\n",
        "\r\n",
        "model.add(Dense(units = 6,                  # 왜 5가 아니라 6일까??\r\n",
        "                activation = 'softmax'))    # 아까 output은 1~5였는데 RNN에서 맨 처음은 0이라서\r\n",
        "                                            # units를 5로 하면 0~4 까지만 검색을 한다.\r\n",
        "                                            # 즉 5번이 나올 수 없다.\r\n",
        "print('RNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN 요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200)               132000    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 453,206\n",
            "Trainable params: 453,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqc7M_MQqf6B"
      },
      "source": [
        "4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWzQhWIeqhbp",
        "outputId": "9fbf4f40-08ea-4d90-9acd-d56bc9d02a6c"
      },
      "source": [
        "# RNN 학습\r\n",
        "model.compile(optimizer = 'adam',\r\n",
        "              loss = 'sparse_categorical_crossentropy',\r\n",
        "              metrics = ['acc'])\r\n",
        "\r\n",
        "print('학습 시작')\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x = X_train,\r\n",
        "          y = Y_train,\r\n",
        "          epochs = MY_EPOCH,\r\n",
        "          verbose = 1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습시간 : {:.2f}초'.format(end - begin))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 시작\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 6s 25ms/step - loss: 1.6741 - acc: 0.2372\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.2982 - acc: 0.3974\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.0508 - acc: 0.5836\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3873 - acc: 0.8961\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3342 - acc: 0.9071\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0689 - acc: 0.9873\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0473 - acc: 0.9943\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0202 - acc: 0.9974\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0104 - acc: 0.9993\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0039 - acc: 0.9992\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 9.0305e-04 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.7928e-04 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.6520e-04 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.3642e-04 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.9523e-04 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.4397e-04 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.2173e-04 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0275 - acc: 0.9929\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1204 - acc: 0.9817\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0201 - acc: 0.9951\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0020 - acc: 0.9998\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0166 - acc: 0.9958\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0164 - acc: 0.9989\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0037 - acc: 0.9998\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.1376e-04 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 7.3202e-04 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.9863e-04 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.4485e-04 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.4845e-04 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.0199e-04 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.5169e-04 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.8042e-04 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.5430e-04 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.5585e-04 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0158 - acc: 0.9950\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.9440e-04 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.0794e-04 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.4746e-04 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.0245e-04 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.7026e-04 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.0838e-04 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.0191e-04 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.8288e-04 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.7863e-04 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.4909e-04 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.5142e-04 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.3509e-04 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.2042e-04 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.1962e-04 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.1098e-04 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.0138e-04 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 8.7626e-05 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 9.2654e-05 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 8.4435e-05 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 8.0668e-05 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.2948e-05 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 7.4359e-05 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.6291e-05 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.6925e-05 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.2468e-05 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 6.0307e-05 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.6430e-05 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.3077e-05 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.0374e-05 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.6883e-05 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 4.5776e-05 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.5500e-05 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.6198e-05 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.3302e-05 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.1617e-05 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.0772 - acc: 0.9773\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.6181e-04 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.5627e-04 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 3.7176e-04 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.9769e-04 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.7611e-04 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 2.0773e-04 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.9734e-04 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.7196e-04 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.5036e-04 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 1.1881e-04 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 1.1901e-04 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.0545e-04 - acc: 1.0000\n",
            "학습시간 : 130.64초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fefJp5XnsjYS"
      },
      "source": [
        "5. 인공 신경망 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lGIzRhGslVg",
        "outputId": "e1db744a-8205-47cb-b094-864e70db61eb"
      },
      "source": [
        "# RNN 평가\r\n",
        "score = model.evaluate(X_test, Y_test,\r\n",
        "                       verbose = 0)\r\n",
        "\r\n",
        "print('최종 손실값 : {:.2f}'.format(score[0]))\r\n",
        "print('최종 정확도 : {:.2f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 손실값 : 0.20\n",
            "최종 정확도 : 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSrwG_LNIxDr"
      },
      "source": [
        "6. 인공 신경망 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDFY8vQ8I5PR",
        "outputId": "6eafb184-8522-4a90-d1bc-ff055272fefb"
      },
      "source": [
        "# RNN 예측\r\n",
        "pred = model.predict(X_test)\r\n",
        "pred = pred.argmax(axis = 1)\r\n",
        "print(pred)\r\n",
        "print(Y_test.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 4 3 1 1 4 2 4 3 5 3 3 2 5 1 5 5 2 1 3 4 2 1 5 4 3 3 1 1 3 2 2 2 2 5 2 3\n",
            " 3 4 4 5 3 5 2 3 1 1 3 4 2 4 1 2 2 3 1 1 3 3 5 5 3 2 3 3 2 4 3 3 3 3 3 5 5\n",
            " 4 3 1 3 1 4 1 1 1 5 4 5 4 1 5 1 1 5 5 2 5 5 3 2 1 4 4 3 2 1 2 5 1 3 5 1 1\n",
            " 2 3 4 4 2 2 1 3 5 1 1 3 5 4 4 5 2 3 1 3 4 5 1 3 2 5 3 5 3 1 3 2 2 3 2 4 1\n",
            " 2 5 2 1 1 5 4 3 4 3 3 1 1 1 2 4 5 2 1 2 1 2 4 2 2 2 2 1 1 1 2 2 5 2 2 2 1\n",
            " 1 1 4 4 1 1 1 2 5 4 4 4 3 2 2 4 2 4 1 1 3 3 3 1 1 3 3 4 2 1 1 1 1 2 1 2 2\n",
            " 2 2 1 3 1 3 4 1 4 2 5 2 1 2 4 4 3 5 2 5 2 4 3 5 2 5 5 4 3 4 4 2 3 1 5 2 3\n",
            " 5 2 4 1 4 3 1 3 2 3 3 2 2 2 4 3 2 3 2 4 3 1 3 3 1 5 4 4 2 4 1 2 2 2 1 4 4\n",
            " 4 1 5 1 3 2 3 3 5 4 2 4 1 5 5 1 2 5 4 4 1 5 2 3 3 3 4 4 2 3 2 4 3 5 1 4 2\n",
            " 4 5 4 4 1 3 1 1 3 5 5 2 3 3 1 2 2 4 2 4 4 1 2 3 1 2 2 1 4 1 4 5 1 1 5 2 4\n",
            " 1 1 3 4 2 3 1 1 3 2 4 4 4 2 1 5 4 4 2 3 4 1 1 4 4 3 2 1 5 5 1 3 4 1 2 2 2\n",
            " 1 1 4 1 2 4 2 2 1 2 3 2 2 4 3 4 3 4 5 3 4 5 4 3 5 2 4 2 4 5 4 1 2 2 3 5 3\n",
            " 1]\n",
            "[5 4 3 1 1 4 2 4 5 5 3 3 2 5 1 5 5 2 1 3 4 2 1 5 4 3 3 1 1 2 2 2 2 2 5 2 3\n",
            " 3 4 4 5 3 5 2 3 1 1 2 4 2 4 1 2 2 3 1 1 3 3 5 5 3 2 3 3 2 4 3 3 3 3 3 5 5\n",
            " 4 3 1 3 1 4 1 1 1 5 4 5 4 1 4 1 1 5 5 2 5 5 3 2 1 4 4 3 2 1 2 5 1 3 5 1 1\n",
            " 2 3 4 4 2 2 1 3 5 1 1 3 5 4 1 5 2 3 1 3 4 5 1 3 2 5 3 5 3 1 3 2 2 3 2 4 1\n",
            " 2 5 2 1 1 5 4 3 4 3 3 1 1 1 2 4 5 2 1 2 1 2 4 2 2 2 2 1 1 1 2 2 5 2 2 2 1\n",
            " 1 1 4 2 1 1 1 2 5 4 4 4 3 2 2 4 2 4 1 1 3 3 3 1 1 3 3 4 2 1 1 1 1 2 1 2 2\n",
            " 2 2 1 3 1 4 4 1 4 2 5 2 1 2 4 4 3 5 2 5 2 4 3 5 3 5 5 4 2 4 4 2 3 1 5 2 3\n",
            " 5 2 4 1 4 3 1 3 2 3 3 2 2 2 4 3 2 3 2 5 3 1 3 3 1 5 4 4 2 4 1 2 2 3 1 4 4\n",
            " 4 1 5 1 3 2 3 3 5 4 2 4 1 5 5 1 2 5 4 4 1 5 2 3 3 3 4 4 2 3 2 3 3 5 1 4 2\n",
            " 4 5 4 4 1 3 1 1 3 5 5 2 3 3 1 2 2 4 2 4 4 1 2 3 1 2 2 1 4 1 4 5 1 1 5 2 4\n",
            " 1 1 3 4 2 3 1 1 3 5 4 4 4 2 1 5 5 4 2 3 4 1 1 4 4 3 2 1 5 5 1 5 4 4 2 2 2\n",
            " 1 1 4 1 2 4 2 2 1 2 3 2 2 4 2 4 3 4 5 3 4 5 1 3 5 2 4 2 4 5 4 1 2 2 3 5 3\n",
            " 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgmEU_xFM6N5",
        "outputId": "f7056ab4-cc52-4f53-d766-4bdbfcfdb1f1"
      },
      "source": [
        "# 혼돈 행렬 출력\r\n",
        "print('혼돈 행렬')\r\n",
        "print(confusion_matrix(y_true = Y_test,\r\n",
        "                       y_pred = pred)) #       1   2   3   4   5 예측\r\n",
        "                                       #  1   95   0   2   2   2\r\n",
        "                                       #  2    0 101   3   2   0\r\n",
        "                                       #  3    0   2  82   2   0\r\n",
        "                                       #  4    1   4   0  80   1\r\n",
        "                                       #  5    0   0   1   1  64\r\n",
        "                                       #  정\r\n",
        "                                       #  답"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "혼돈 행렬\n",
            "[[ 99   0   0   2   0]\n",
            " [  0 101   4   1   0]\n",
            " [  0   2  83   1   0]\n",
            " [  1   0   1  83   1]\n",
            " [  0   1   2   2  61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuFbgOLENymA",
        "outputId": "db626a34-b483-4400-e7d1-4f051dbca4ca"
      },
      "source": [
        "# 실제 기사 분류\r\n",
        "news = ['Paul Pogbas second-half volley was enough to give Manchester United victory at Burnley and send them three points clear at the top of the Premier League.United dominated a contest in which Burnley failed to register a single shot on target until stoppage time.But they were struggling to make a breakthrough until Marcus Rashford picked Pogba out with an excellent cross to the edge of the area.The Frenchmans connection was perfect, although it took a deflection off Matthew Lowton to ensure the ball went past Nick Pope and into the Burnley net.Although Burnley had three decent chances in a frantic ending, United secured the win to head the table after 17 rounds of matches.']\r\n",
        "\r\n",
        "news = A_token.texts_to_sequences(news)\r\n",
        "print(news)\r\n",
        "\r\n",
        "news = pad_sequences(news,\r\n",
        "                     maxlen = MY_LEN,\r\n",
        "                     padding = 'post',\r\n",
        "                     truncating = 'post')\r\n",
        "#print(news)\r\n",
        "\r\n",
        "pred = model.predict(news)\r\n",
        "pred = pred.argmax(axis = 1)\r\n",
        "print('RNN 추측값 :', pred)\r\n",
        "\r\n",
        "# {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[630, 1, 64, 104, 4061, 1, 381, 605, 208, 658, 190, 434, 3380, 4401, 1770, 1051, 597, 31, 549, 373, 3380, 1219, 66, 1332, 1219, 2569, 463, 190, 1446, 1364, 1750, 619, 1, 4401, 534, 605, 2992, 1364, 503, 811, 578, 760, 1, 1, 14, 1, 1, 1, 1849, 605, 21, 1364, 2534, 1, 1, 1, 1309, 1, 569, 2404, 1, 2561, 872, 605, 1219, 2177, 1332, 1219, 826, 1219, 1, 1581, 1, 2599, 238, 221, 170, 1364, 1, 813, 2821, 1, 605, 660, 1219, 692, 293, 255, 2853, 1, 1770, 1, 1219, 4401, 115, 238, 4401, 3728, 31, 3291, 1370, 619, 1364, 1, 3342, 190, 2627, 1219, 58, 605, 392, 1219, 2367, 1, 579, 1, 1332, 1251]]\n",
            "RNN 추측값 : [1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}